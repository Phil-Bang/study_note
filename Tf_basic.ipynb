{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서플로우의 기본적인 구성을 익힙니다.\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# tf.constant: 말 그대로 상수입니다.\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "print(hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(10)\n",
    "b = tf.constant(32)\n",
    "c = tf.add(a, b)  # a + b 로도 쓸 수 있음\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n",
      "[10, 32, 42]\n"
     ]
    }
   ],
   "source": [
    "# 위에서 변수와 수식들을 정의했지만, 실행이 정의한 시점에서 실행되는 것은 아닙니다.\n",
    "# 다음처럼 Session 객제와 run 메소드를 사용할 때 계산이 됩니다.\n",
    "# 따라서 모델을 구성하는 것과, 실행하는 것을 분리하여 프로그램을 깔끔하게 작성할 수 있습니다.\n",
    "# 그래프를 실행할 세션을 구성합니다.\n",
    "sess = tf.Session()\n",
    "# sess.run: 설정한 텐서 그래프(변수나 수식 등등)를 실행합니다.\n",
    "print(sess.run(hello))\n",
    "print(sess.run([a, b, c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세션을 닫습니다.\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 플레이스홀더와 변수의 개념을 익혀봅니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tf.placeholder: 계산을 실행할 때 입력값을 받는 변수로 사용합니다.\n",
    "# None 은 크기가 정해지지 않았음을 의미합니다.\n",
    "X = tf.placeholder(tf.float32,[None,3])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X 플레이스홀더에 넣을 값 입니다.\n",
    "# 플레이스홀더에서 설정한 것 처럼, 두번째 차원의 요소의 갯수는 3개 입니다.\n",
    "x_data = [[1, 2, 3], [4, 5, 6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# tf.Variable:그래프를 계산하면서 최적화 할 변수들입니다. 이 값이 바로 신경망을 좌우하는 값들입니다.\n",
    "# tf.random_normal:각 변수들의 초기값을 정규분포 랜덤값으로 초기화합니다.\n",
    "W = tf.Variable(tf.random_normal([3,2]))\n",
    "b = tf.Variable(tf.random_normal([2,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력값과 변수들을 계산할 수식을 작성합니다.\n",
    "# tf.matmul처럼 mat*로 되어 있는 함수로 행렬 계산을 수행합니다.\n",
    "expr = tf.matmul(X,W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== x_data ===\n",
      "[[1, 2, 3], [4, 5, 6]]\n",
      "=== W ===\n",
      "[[-0.66322744 -2.3054585 ]\n",
      " [ 0.26017693  0.92784286]\n",
      " [-0.7922577   0.2714553 ]]\n",
      "=== b ===\n",
      "[[-1.854476 ]\n",
      " [ 0.5582989]]\n",
      "=== expr ===\n",
      "[[-4.3741226 -1.489883 ]\n",
      " [-5.547272  -2.395589 ]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "# 위에서 설정한 Variable 들의 값들을 초기화 하기 위해\n",
    "# 처음에 tf.global_variables_initializer 를 한 번 실행해야 합니다.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"=== x_data ===\")\n",
    "print(x_data)\n",
    "print(\"=== W ===\")\n",
    "print(sess.run(W))\n",
    "print(\"=== b ===\")\n",
    "print(sess.run(b))\n",
    "print(\"=== expr ===\")\n",
    "# expr 수식에는 X 라는 입력값이 필요합니다.\n",
    "# 따라서 expr 실행시에는 이 변수에 대한 실제 입력값을 다음처럼 넣어줘야합니다.\n",
    "print(sess.run(expr, feed_dict={X: x_data}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X 와 Y 의 상관관계를 분석하는 기초적인 선형 회귀 모델을 만들고 실행해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"X:0\", dtype=float32)\n",
      "Tensor(\"Y:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# name: 나중에 텐서보드등으로 값의 변화를 추적하거나 살펴보기 쉽게 하기 위해 이름을 붙여줍니다.\n",
    "X = tf.placeholder(tf.float32, name=\"X\")\n",
    "Y = tf.placeholder(tf.float32, name=\"Y\")\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X와 Y의 상관관계를 분석하기 위한 가설수식을 작성\n",
    "# y = W * x + b\n",
    "# W와 X가 행렬이 아니므로 tf.matmul이 아니라 기본곱셈기호를 사용\n",
    "hypothesis = W * X + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# 손실함수 작성\n",
    "# mean(h-Y)^2 :예측값과 실제값의 거리를 비용(손실)함수로 정합니다.\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "# 텐서플로우에 기본적으로 포함되어 있는 함수를 이용해 경사하강법 최적화를 수행\n",
    "# 텐서플로우에게 학습도를 0.01로 준 경사 하강법 알고리즘을 실행한다.\n",
    "# 학습률의 값이 너무 크거나 작으면 적합도가 좋은 곳을 찾아갈 수 없기 때문에 보통 이 학습률의 값을 조정해가면서\n",
    "# 올바르게 학습이 진행되고 있는지 계속해서 확인하고 변경하게 된다\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "# 비용을 최소화하는 것이 최종목표\n",
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 16.95501 [1.1947577] [0.03183037]\n",
      "1 0.20281924 [1.0002518] [-0.05243877]\n",
      "2 0.0026973088 [1.0209923] [-0.04205173]\n",
      "3 0.00029378754 [1.0182202] [-0.0420383]\n",
      "4 0.0002526548 [1.0180299] [-0.04091872]\n",
      "5 0.00024032628 [1.0175695] [-0.03994694]\n",
      "6 0.00022890675 [1.01715] [-0.03898536]\n",
      "7 0.00021803653 [1.0167375] [-0.03804832]\n",
      "8 0.00020767807 [1.0163351] [-0.03713365]\n",
      "9 0.00019781334 [1.0159425] [-0.03624097]\n",
      "10 0.00018841517 [1.0155592] [-0.03536974]\n",
      "11 0.00017946736 [1.0151851] [-0.03451948]\n",
      "12 0.00017094181 [1.0148201] [-0.03368964]\n",
      "13 0.0001628205 [1.0144639] [-0.03287974]\n",
      "14 0.0001550871 [1.0141162] [-0.03208935]\n",
      "15 0.00014772071 [1.0137768] [-0.03131795]\n",
      "16 0.00014070373 [1.0134456] [-0.03056507]\n",
      "17 0.00013402048 [1.0131224] [-0.0298303]\n",
      "18 0.00012765378 [1.012807] [-0.02911322]\n",
      "19 0.00012159106 [1.0124991] [-0.02841338]\n",
      "20 0.000115815434 [1.0121986] [-0.02773035]\n",
      "21 0.00011031365 [1.0119054] [-0.02706371]\n",
      "22 0.00010507469 [1.0116192] [-0.02641314]\n",
      "23 0.00010008336 [1.0113399] [-0.0257782]\n",
      "24 9.532816e-05 [1.0110673] [-0.02515852]\n",
      "25 9.0800786e-05 [1.0108012] [-0.02455372]\n",
      "26 8.648672e-05 [1.0105416] [-0.02396344]\n",
      "27 8.2379906e-05 [1.0102881] [-0.02338738]\n",
      "28 7.8465295e-05 [1.0100409] [-0.02282514]\n",
      "29 7.473888e-05 [1.0097995] [-0.02227646]\n",
      "30 7.118927e-05 [1.0095639] [-0.02174097]\n",
      "31 6.780768e-05 [1.009334] [-0.02121835]\n",
      "32 6.458626e-05 [1.0091096] [-0.02070826]\n",
      "33 6.151904e-05 [1.0088906] [-0.02021046]\n",
      "34 5.8596313e-05 [1.0086769] [-0.01972461]\n",
      "35 5.5812594e-05 [1.0084683] [-0.01925044]\n",
      "36 5.3162566e-05 [1.0082647] [-0.01878767]\n",
      "37 5.063723e-05 [1.008066] [-0.01833601]\n",
      "38 4.823068e-05 [1.0078722] [-0.01789522]\n",
      "39 4.594089e-05 [1.0076829] [-0.01746507]\n",
      "40 4.375851e-05 [1.0074983] [-0.01704523]\n",
      "41 4.167994e-05 [1.007318] [-0.01663548]\n",
      "42 3.970032e-05 [1.0071421] [-0.0162356]\n",
      "43 3.781426e-05 [1.0069704] [-0.0158453]\n",
      "44 3.6018107e-05 [1.0068028] [-0.01546441]\n",
      "45 3.430768e-05 [1.0066392] [-0.01509265]\n",
      "46 3.267775e-05 [1.0064796] [-0.01472982]\n",
      "47 3.1125077e-05 [1.0063239] [-0.0143757]\n",
      "48 2.964715e-05 [1.0061718] [-0.01403014]\n",
      "49 2.8238703e-05 [1.0060235] [-0.01369285]\n",
      "50 2.6897595e-05 [1.0058787] [-0.0133637]\n",
      "51 2.5619991e-05 [1.0057374] [-0.01304243]\n",
      "52 2.4403062e-05 [1.0055995] [-0.01272892]\n",
      "53 2.3243616e-05 [1.0054649] [-0.01242294]\n",
      "54 2.2138876e-05 [1.0053335] [-0.0121243]\n",
      "55 2.1088235e-05 [1.0052053] [-0.01183287]\n",
      "56 2.008641e-05 [1.0050801] [-0.0115484]\n",
      "57 1.9131461e-05 [1.004958] [-0.01127075]\n",
      "58 1.8222974e-05 [1.0048388] [-0.0109998]\n",
      "59 1.7357002e-05 [1.0047226] [-0.01073536]\n",
      "60 1.6533259e-05 [1.004609] [-0.01047733]\n",
      "61 1.574749e-05 [1.0044982] [-0.01022546]\n",
      "62 1.4999696e-05 [1.0043901] [-0.00997966]\n",
      "63 1.4286961e-05 [1.0042846] [-0.00973976]\n",
      "64 1.3608507e-05 [1.0041816] [-0.00950565]\n",
      "65 1.2962114e-05 [1.004081] [-0.00927717]\n",
      "66 1.2346806e-05 [1.0039829] [-0.00905414]\n",
      "67 1.1759811e-05 [1.0038872] [-0.00883646]\n",
      "68 1.1201403e-05 [1.0037937] [-0.00862403]\n",
      "69 1.0669294e-05 [1.0037025] [-0.00841671]\n",
      "70 1.0162437e-05 [1.0036135] [-0.00821438]\n",
      "71 9.6795575e-06 [1.0035267] [-0.00801689]\n",
      "72 9.2201735e-06 [1.0034418] [-0.00782418]\n",
      "73 8.782029e-06 [1.0033591] [-0.00763607]\n",
      "74 8.364837e-06 [1.0032784] [-0.00745249]\n",
      "75 7.967697e-06 [1.0031996] [-0.00727335]\n",
      "76 7.589e-06 [1.0031227] [-0.00709851]\n",
      "77 7.228544e-06 [1.0030476] [-0.00692787]\n",
      "78 6.8852023e-06 [1.0029743] [-0.00676134]\n",
      "79 6.558121e-06 [1.0029029] [-0.00659878]\n",
      "80 6.2466115e-06 [1.002833] [-0.00644017]\n",
      "81 5.9500903e-06 [1.0027649] [-0.00628535]\n",
      "82 5.66725e-06 [1.0026985] [-0.00613425]\n",
      "83 5.398353e-06 [1.0026336] [-0.00598682]\n",
      "84 5.1415677e-06 [1.0025703] [-0.00584288]\n",
      "85 4.8975544e-06 [1.0025085] [-0.00570241]\n",
      "86 4.6646387e-06 [1.0024482] [-0.00556533]\n",
      "87 4.4433214e-06 [1.0023893] [-0.00543155]\n",
      "88 4.2318884e-06 [1.002332] [-0.00530095]\n",
      "89 4.0311165e-06 [1.0022758] [-0.00517354]\n",
      "90 3.839696e-06 [1.0022211] [-0.00504916]\n",
      "91 3.657184e-06 [1.0021677] [-0.00492777]\n",
      "92 3.483375e-06 [1.0021156] [-0.00480929]\n",
      "93 3.3177555e-06 [1.0020648] [-0.00469366]\n",
      "94 3.1605093e-06 [1.0020151] [-0.00458086]\n",
      "95 3.0101971e-06 [1.0019667] [-0.00447073]\n",
      "96 2.8671973e-06 [1.0019194] [-0.00436327]\n",
      "97 2.7311714e-06 [1.0018733] [-0.00425838]\n",
      "98 2.601415e-06 [1.0018282] [-0.004156]\n",
      "99 2.4776955e-06 [1.0017843] [-0.00405607]\n",
      "\n",
      "=== Test ===\n",
      "X: 5, Y: [5.0048656]\n",
      "X: 2.5, Y: [2.5004048]\n"
     ]
    }
   ],
   "source": [
    "# 세션을 생성하고 초기화\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # 최적화를 100번 수행합니다.\n",
    "    for step in range(100):\n",
    "        # sess.run 을 통해 train_op 와 cost 그래프를 계산합니다.\n",
    "        # 이 때, 가설 수식에 넣어야 할 실제값을 feed_dict 을 통해 전달합니다.\n",
    "        _, cost_val = sess.run([train_op, cost], feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "        print(step, cost_val, sess.run(W), sess.run(b))\n",
    "\n",
    "    # 최적화가 완료된 모델에 테스트 값을 넣고 결과가 잘 나오는지 확인해봅니다.\n",
    "    print(\"\\n=== Test ===\")\n",
    "    print(\"X: 5, Y:\", sess.run(hypothesis, feed_dict={X: 5}))\n",
    "    print(\"X: 2.5, Y:\", sess.run(hypothesis, feed_dict={X: 2.5}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 털과 날개가 있는지 없는지에 따라, 포유류인지 조류인지 분류하는 신경망 모델을 만들어봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [털, 날개]\n",
    "x_data = np.array([[0, 0], [1, 0], [1, 1], [0, 0], [0, 0], [0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [기타, 포유류, 조류]\n",
    "# 다음과 같은 형식을 one-hot 형식의 데이터라고 합니다.\n",
    "y_data = np.array([\n",
    "    [1, 0, 0],  # 기타\n",
    "    [0, 1, 0],  # 포유류\n",
    "    [0, 0, 1],  # 조류\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망은 2차원으로 [입력층(특성),출력층(레이블)] -> [2,3]으로 정합니다.\n",
    "W = tf.Variable(tf.random_uniform([2,3],-1.,1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 편향을 각각 각 레이어의 아웃풋 갯수로 설정\n",
    "# 편향은 아웃풋의 갯수, 즉 최종결과값의 분류갯수인 3으로 설정\n",
    "b = tf.Variable(tf.zeros([3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망에 가중치 W과 편향 b을 적용합니다\n",
    "L = tf.add(tf.matmul(X, W), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치와 편향을 이용해 계산한 결과 값에\n",
    "# 텐서플로우에서 기본적으로 제공하는 활성화 함수인 ReLU 함수를 적용합니다.\n",
    "L = tf.nn.relu(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마지막으로 softmax함수를 이용하여 출력값을 사용하기 쉽게 만듭니다\n",
    "# softmax함수는 다음처럼 결과값을 전체합이 1인 확률로 만들어주는 함수입니다.\n",
    "# 예) [8.04, 2.76, -6.52] -> [0.53 0.24 0.23]\n",
    "model = tf.nn.softmax(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망을 최적화하기 위한 비용함수를 작성합니다.\n",
    "# 각 개별결과에 대한 합을 구한 뒤 평균을 내는 방식을 사용합니다.\n",
    "# 전체합이 아닌, 개별결과를 구한 뒤 평균을 내는 방식을 사용하기 위해 axis옵션을 사용합니다.\n",
    "# axis옵션이 없으면 -1.09처럼 총합인 스칼라값으로 출력됩니다.\n",
    "#        Y         model         Y * tf.log(model)   reduce_sum(axis=1)\n",
    "# 예) [[1 0 0]  [[0.1 0.7 0.2]  -> [[-1.0  0    0]  -> [-1.0, -0.09]\n",
    "#     [0 1 0]]  [0.2 0.8 0.0]]     [ 0   -0.09 0]]\n",
    "# 즉, 이것은 예측값과 실제값 사이의 확률 분포의 차이를 비용으로 계산한 것이며,\n",
    "# 이것을 Cross-Entropy 라고 합니다.\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(model), axis=1))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  신경망 모델학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1.21361\n",
      "20 1.2069219\n",
      "30 1.2004808\n",
      "40 1.1942776\n",
      "50 1.1883036\n",
      "60 1.1825496\n",
      "70 1.1770073\n",
      "80 1.1716682\n",
      "90 1.1665243\n",
      "100 1.1615677\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(100):\n",
    "    sess.run(train_op, feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(step + 1, sess.run(cost, feed_dict={X: x_data, Y: y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ArgMax:0' shape=(?,) dtype=int64>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########\n",
    "# 결과 확인\n",
    "# 0:기타 1:포유류, 2:조류\n",
    "######\n",
    "# tf.argmax: 예측값과 실제값의 행렬에서 tf.argmax를 이용해 가장 큰 값을 가져옵니다.\n",
    "# 예) [[0 1 0] [1 0 0]] -> [1 0]\n",
    "#    [[0.2 0.7 0.1] [0.9 0.1 0.]] -> [1 0]\n",
    "prediction = tf.argmax(model, 1)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ArgMax_1:0' shape=<unknown> dtype=int64>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = tf.argmax(Y, 1)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값: [0 0 0 0 0 0]\n",
      "실제값: [0 1 2 0 0 2]\n"
     ]
    }
   ],
   "source": [
    "print('예측값:', sess.run(prediction, feed_dict={X: x_data}))\n",
    "print('실제값:', sess.run(target, feed_dict={Y: y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Equal_1:0' shape=<unknown> dtype=bool>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_correct = tf.equal(prediction, target)\n",
    "is_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mean_2:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 50.00\n"
     ]
    }
   ],
   "source": [
    "print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: x_data, Y: y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec 모델을 간단하게 구현해봅니다.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 그래프를 노트북 안에 그리기 위해 설정\n",
    "%matplotlib inline\n",
    "\n",
    "# 필요한 패키지와 라이브러리를 가져옴\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# 그래프에서 마이너스 폰트 깨지는 문제에 대한 대처\n",
    "mpl.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplot에서 한글을 표시하기 위한 설정\n",
    "font_name = matplotlib.font_manager.FontProperties(\n",
    "                fname='C:/ProgramData/Anaconda3/Lib/site-packages/matplotlib/mpl-data/fonts/ttf/NanumGothic.ttf'  # 한글 폰트 위치를 넣어주세요\n",
    "            ).get_name()\n",
    "matplotlib.rc('font',family=font_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 벡터를 분석해볼 임의의 문장들\n",
    "sentences = [\"나 고양이 좋다\",\n",
    "             \"나 강아지 좋다\",\n",
    "             \"나 동물 좋다\",\n",
    "             \"강아지 고양이 동물\",\n",
    "             \"여자친구 고양이 강아지 좋다\",\n",
    "             \"고양이 생선 우유 좋다\",\n",
    "             \"강아지 생선 싫다 우유 좋다\",\n",
    "             \"강아지 고양이 눈 좋다\",\n",
    "             \"나 여자친구 좋다\",\n",
    "             \"여자친구 나 싫다\",\n",
    "             \"여자친구 나 영화 책 음악 좋다\",\n",
    "             \"나 게임 만화 애니 좋다\",\n",
    "             \"고양이 강아지 싫다\",\n",
    "             \"강아지 고양이 좋다\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 전부 합친 후 공백으로 단어들을 나누고 고유한 단어들로 리스트를 만듭니다.\n",
    "word_sequence = \" \".join(sentences).split()\n",
    "word_list = \" \".join(sentences).split()\n",
    "word_list = list(set(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자열로 분석하는 것 보다, 숫자로 분석하는 것이 훨씬 용이하므로\n",
    "# 리스트에서 문자들의 인덱스를 뽑아서 사용하기 위해,\n",
    "# 이를 표현하기 위한 연관 배열과, 단어 리스트에서 단어를 참조 할 수 있는 인덱스 배열을 만듭합니다.\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 윈도우 사이즈를 1로 하는 skip-gram 모델을 만듭니다.\n",
    "# 예) 나 게임 만화 애니 좋다\n",
    "#   -> ([나, 만화], 게임), ([게임, 애니], 만화), ([만화, 좋다], 애니)\n",
    "#   -> (게임, 나), (게임, 만화), (만화, 게임), (만화, 애니), (애니, 만화), (애니, 좋다)\n",
    "skip_grams = []\n",
    "\n",
    "for i in range(1, len(word_sequence) - 1):\n",
    "    # (context, target) : ([target index - 1, target index + 1], target)\n",
    "    # 스킵그램을 만든 후, 저장은 단어의 고유 번호(index)로 저장합니다\n",
    "    target = word_dict[word_sequence[i]]\n",
    "    context = [word_dict[word_sequence[i - 1]], word_dict[word_sequence[i + 1]]]\n",
    "\n",
    "    # (target, context[0]), (target, context[1])..\n",
    "    for w in context:\n",
    "        skip_grams.append([target, w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip-gram 데이터에서 무작위로 데이터를 뽑아 입력값과 출력값의 배치 데이터를 생성하는 함수\n",
    "def random_batch(data, size):\n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(data)), size, replace=False)\n",
    "\n",
    "    for i in random_index:\n",
    "        random_inputs.append(data[i][0])  # target\n",
    "        random_labels.append([data[i][1]])  # context word\n",
    "\n",
    "    return random_inputs, random_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# 옵션 설정\n",
    "######\n",
    "# 학습을 반복할 횟수\n",
    "training_epoch = 300\n",
    "# 학습률\n",
    "learning_rate = 0.1\n",
    "# 한 번에 학습할 데이터의 크기\n",
    "batch_size = 20\n",
    "# 단어 벡터를 구성할 임베딩 차원의 크기\n",
    "# 이 예제에서는 x, y 그래프로 표현하기 쉽게 2 개의 값만 출력하도록 합니다.\n",
    "embedding_size = 2\n",
    "# word2vec 모델을 학습시키기 위한 nce_loss 함수에서 사용하기 위한 샘플링 크기\n",
    "# batch_size 보다 작아야 합니다.\n",
    "num_sampled = 15\n",
    "# 총 단어 갯수\n",
    "voc_size = len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "inputs = tf.placeholder(tf.int32, shape=[batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.nn.nce_loss 를 사용하려면 출력값을 이렇게 [batch_size, 1] 구성해야합니다.\n",
    "labels = tf.placeholder(tf.int32, shape=[batch_size, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec 모델의 결과 값인 임베딩 벡터를 저장할 변수입니다.\n",
    "# 총 단어 갯수와 임베딩 갯수를 크기로 하는 두 개의 차원을 갖습니다.\n",
    "embeddings = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 벡터의 차원에서 학습할 입력값에 대한 행들을 뽑아옵니다.\n",
    "# 예) embeddings     inputs    selected\n",
    "#    [[1, 2, 3]  -> [2, 3] -> [[2, 3, 4]\n",
    "#     [2, 3, 4]                [3, 4, 5]]\n",
    "#     [3, 4, 5]\n",
    "#     [4, 5, 6]]\n",
    "selected_embed = tf.nn.embedding_lookup(embeddings, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nce_loss 함수에서 사용할 변수들을 정의합니다.\n",
    "nce_weights = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\n",
    "nce_biases = tf.Variable(tf.zeros([voc_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nce_loss 함수를 직접 구현하려면 매우 복잡하지만,\n",
    "# 함수를 텐서플로우가 제공하므로 그냥 tf.nn.nce_loss 함수를 사용하기만 하면 됩니다.\n",
    "loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(nce_weights, nce_biases, labels, selected_embed, num_sampled, voc_size))\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step  10 :  5.919656\n",
      "loss at step  20 :  3.9946074\n",
      "loss at step  30 :  3.3942344\n",
      "loss at step  40 :  3.4237072\n",
      "loss at step  50 :  3.2209\n",
      "loss at step  60 :  3.162049\n",
      "loss at step  70 :  3.1379917\n",
      "loss at step  80 :  3.2183833\n",
      "loss at step  90 :  3.0083623\n",
      "loss at step  100 :  3.1489146\n",
      "loss at step  110 :  3.5853043\n",
      "loss at step  120 :  3.2198384\n",
      "loss at step  130 :  3.3655007\n",
      "loss at step  140 :  3.065447\n",
      "loss at step  150 :  2.967588\n",
      "loss at step  160 :  3.3667789\n",
      "loss at step  170 :  3.291223\n",
      "loss at step  180 :  2.9558861\n",
      "loss at step  190 :  2.8711343\n",
      "loss at step  200 :  3.2538018\n",
      "loss at step  210 :  3.3301873\n",
      "loss at step  220 :  3.408256\n",
      "loss at step  230 :  2.9807675\n",
      "loss at step  240 :  3.3608787\n",
      "loss at step  250 :  3.1943593\n",
      "loss at step  260 :  2.8957705\n",
      "loss at step  270 :  3.0340564\n",
      "loss at step  280 :  2.7595305\n",
      "loss at step  290 :  2.9654286\n",
      "loss at step  300 :  3.238135\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, training_epoch + 1):\n",
    "        batch_inputs, batch_labels = random_batch(skip_grams, batch_size)\n",
    "\n",
    "        _, loss_val = sess.run([train_op, loss],\n",
    "                               feed_dict={inputs: batch_inputs,\n",
    "                                          labels: batch_labels})\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(\"loss at step \", step, \": \", loss_val)\n",
    "\n",
    "    # matplot 으로 출력하여 시각적으로 확인해보기 위해\n",
    "    # 임베딩 벡터의 결과 값을 계산하여 저장합니다.\n",
    "    # with 구문 안에서는 sess.run 대신 간단히 eval() 함수를 사용할 수 있습니다.\n",
    "    trained_embeddings = embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#########\n",
    "# 임베딩된 Word2Vec 결과 확인\n",
    "# 결과는 해당 단어들이 얼마나 다른 단어와 인접해 있는지를 보여줍니다.\n",
    "######\n",
    "\n",
    "for i, label in enumerate(word_list):\n",
    "    x, y = trained_embeddings[i]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label, xy=(x, y), xytext=(5, 2),\n",
    "                 textcoords='offset points', ha='right', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
